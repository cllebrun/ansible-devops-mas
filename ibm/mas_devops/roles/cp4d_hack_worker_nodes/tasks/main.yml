---
# This is a horrible hack, but it's what CP4D tells us to do :(
#
# This should be executed as part of cluster prepararion if you want to use CP4D v4.
#
# It will reboot all worker nodes, causing disruption to the entire cluster and everything
# running on it we do not include this as part of the normal flow because, well it shouldn't
# be necessary to reboot worker nodes to install containerized software.  Hopefully this is just
# an example of poor documentation and there's a simple alternative that we can implement to
# remove this role.
#
# https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret


# 0. Fail if required params are not provided
# -----------------------------------------------------------------------------
- name: "Fail if cpd_entitlement_key has not been provided"
  assert:
    that: cpd_entitlement_key is defined and cpd_entitlement_key != ""
    fail_msg: "cpd_entitlement_key property has not been set"

- name: "Fail if cluster_type is not set to roks"
  assert:
    that: cluster_type is defined and cluster_type == "roks"
    fail_msg: "cluster_type property has not been set or is not set to roks, only 'roks' is supported by this role today"

- name: "Fail if cluster_name has not been provided"
  assert:
    that: cluster_name is defined and cluster_name != ""
    fail_msg: "cluster_name property has not been set"

- name: "Fail if ibmcloud_apikey has not been provided"
  assert:
    that: ibmcloud_apikey is defined and ibmcloud_apikey != ""
    fail_msg: "ibmcloud_apikey property has not been set"


- name: "Debug information"
  debug:
    msg:
      - "CPD registry ................. {{ cpd_registry }}"
      - "CPD username ................. {{ cpd_registry_user }}"
      - "Cluster Name ................. {{ cluster_name }}"
      - "Cluster Type ................. {{ cluster_type }}"


# 1. Update cp4d entitlement key and cpd registry server in cluster's pull-secret
# ---------------------------------------------------------------------------------------------------------------------

# 1.1 Generate the new secret
- name: Set new secret content
  vars:
    entitledAuthStr: "{{ cpd_registry_user }}:{{ cpd_entitlement_key }}"
    entitledAuth: "{{ entitledAuthStr | b64encode }}"
    content:
      - "{\"auths\":{\"{{ cpd_registry }}\":{\"username\":\"{{ cpd_registry_user }}\",\"password\":\"{{ cpd_entitlement_key }}\",\"email\":\"{{ cpd_registry_user }}\",\"auth\":\"{{ entitledAuth }}\"}"
      - "}"
      - "}"
  set_fact:
    new_secret: "{{ content | join('') }}"

# 1.2 Find the existing secret, and we are going to modify it rather than replace
- name: Retrieve existing pull-secret content
  kubernetes.core.k8s_info:
    api: v1
    kind: Secret
    name: pull-secret
    namespace: openshift-config
  register: pullsecret

- name: Get the original cred secrets
  set_fact:
    original_secret: "{{ item.data }}"
  with_items: "{{ pullsecret.resources }}"
  no_log: true

- name: Get the dockerconfigjson info
  set_fact:
    secret_string: '{{ original_secret[".dockerconfigjson"] | b64decode | from_json }}'

# 1.3 Append our new credentials to the secret
- name: Combine new secret content
  set_fact:
    new_secret_string: '{{ secret_string | combine( new_secret, recursive=True) }}'

# 1.4. Overwrite the secret
- name: "Update new pull-secret"
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/dockerconfigjson
      metadata:
        name: pull-secret
        namespace: openshift-config
      data:
        .dockerconfigjson: "{{ new_secret_string | to_json | b64encode }}"
  register: secretUpdateResult

- name: Debug change state
  debug:
    msg: "Default Pull Secret Changed ....... {{ secretUpdateResult.changed }}"


# 2. Reload the cluster worker nodes **if the pull-secret was changed**
# ---------------------------------------------------------------------------------------------------------------------

# 2.1 Get cluster info
- name: "Get cluster worker node list"
  when: secretUpdateResult.changed
  shell: ibmcloud oc worker ls -c {{ cluster_name }} -q | awk '{print $1}'
  register: cluster_lookup
  failed_when: "cluster_lookup.rc > 1"

# 2.2 Issue reload command to the cluster's worker nodes
- name: "Reload all worker nodes"
  vars:
    workers: "{{ item }}"
  when:
    - secretUpdateResult.changed
    - item | length > 0
  shell: ibmcloud oc worker reload -c {{ cluster_name }} -w {{ item }} -f
  with_items: "{{ cluster_lookup.stdout_lines }}"

# 2.3 Wait for cluster status to enter "pending"
- name: "Wait until the ROKS cluster status is 'pending' (30 minute timeout)"
  when: secretUpdateResult.changed
  shell: ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'pending'
  retries: 30
  delay: 60

# 2.4 Wait for cluster status to return to "normal"
- name: "Wait until the ROKS cluster status is 'normal' again (1 hour timeout)"
  when: secretUpdateResult.changed
  shell: ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'normal'
  retries: 60
  delay: 60
